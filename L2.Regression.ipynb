{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ML-Challenge/week3-supervised-learning/blob/master/L2.Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download lesson datasets\n",
    "# Required if you're using Google Colab\n",
    "!wget \"https://github.com/ML-Challenge/week3-supervised-learning/raw/master/datasets.zip\"\n",
    "!unzip -o datasets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:49.136768Z",
     "start_time": "2020-02-06T18:03:48.984208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# and setting the size of all plots.\n",
    "plt.rcParams['figure.figsize'] = [11, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we used image and political datasets to predict binary and multiclass outcomes. But what if our problem requires a continuous outcome? Regression is best suited to solving such problems. We will learn about fundamental regression concepts and apply them to predict the life expectancy in a given country using Gapminder data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we're going to check out the other type of supervised learning problem: regression. In regression tasks, the target value is a continuously varying variable, such as a country's GDP or the price of a house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with [Gapminder](https://www.gapminder.org/data/) data, consolidated into one CSV file available in the as `'gapminder.csv'`. Our goal is to use this data to predict the life expectancy in a given country based on features such as the country's GDP, fertility rate, and population. As in Lesson 1, the dataset has been preprocessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the target variable here is quantitative, this is a regression problem. To begin, we will fit a linear regression model with just one feature: `'fertility'` (the average number of children born to a woman in a given country). In later examples, we will use all the features to build regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before that, however, we need to import the data and get it into the form needed by scikit-learn. This involves creating features and target variable arrays. Furthermore, since we are going to use only one feature, for now, we need to do some reshaping using NumPy's `.reshape()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:50.587144Z",
     "start_time": "2020-02-06T18:03:50.583155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:50.775261Z",
     "start_time": "2020-02-06T18:03:50.762296Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame: df\n",
    "gapminder = pd.read_csv('data/gapminder.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:50.930992Z",
     "start_time": "2020-02-06T18:03:50.923980Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create arrays for features and target variable\n",
    "y = gapminder['life'].values\n",
    "X = gapminder['fertility'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:51.120089Z",
     "start_time": "2020-02-06T18:03:51.106097Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the dimensions of X and y before reshaping\n",
    "print(\"Dimensions of y before reshaping: {}\".format(y.shape))\n",
    "print(\"Dimensions of X before reshaping: {}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:51.278531Z",
     "start_time": "2020-02-06T18:03:51.263578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reshape X and y\n",
    "y = y.reshape(-1,1)\n",
    "X = X.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:51.450219Z",
     "start_time": "2020-02-06T18:03:51.444236Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the dimensions of X and y after reshaping\n",
    "print(\"Dimensions of y after reshaping: {}\".format(y.shape))\n",
    "print(\"Dimensions of X after reshaping: {}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Gapminder data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, it is important to explore the data before building models. Below, we have constructed a heatmap showing the correlation between the different features of the Gapminder dataset. Cells that are in green show positive correlation, while cells that are in red show a negative correlation. Take a moment to explore this: Which features are positively correlated with life, and which ones are negatively correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:52.493917Z",
     "start_time": "2020-02-06T18:03:51.957038Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# df.corr() computes the pairwise correlation between columns\n",
    "sns.heatmap(gapminder.corr(), square=True, cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the DataFrame using pandas methods such as `.info()`, `.describe()`, `.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:52.634921Z",
     "start_time": "2020-02-06T18:03:52.621932Z"
    }
   },
   "outputs": [],
   "source": [
    "gapminder.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:52.806469Z",
     "start_time": "2020-02-06T18:03:52.761583Z"
    }
   },
   "outputs": [],
   "source": [
    "gapminder.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:52.946037Z",
     "start_time": "2020-02-06T18:03:52.931048Z"
    }
   },
   "outputs": [],
   "source": [
    "gapminder.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basics of linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does linear regression actually work? We want to fit a line to the data, and a line in two dimensions is always of the form `y = ax + b`, where `y` is the target, `x` is the single feature, and `a` and `b` are the parameters of the model that we want to learn. So the questions of `fitting` is reduced to how do we choose `a` and `b`?\n",
    "\n",
    "A common method is to define an error function for any given line and then to choose the line that minimizes the error function. Such an error function is also called a `loss` or a `cost` function. What will our loss function be?\n",
    "\n",
    "![Data](assets/loss_function_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we want a line to be as close to the actual data points as possible.\n",
    "\n",
    "![Line](assets/loss_function_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this reason, we wish to minimize the vertical distance between the fit and the data. So for each data point, we calculate the vertical distance between it and the line.\n",
    "\n",
    "![Distances](assets/loss_function_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distance is called a residual. We can try to minimize the sum of the residuals, but then a large positive residual would cancel out a large negative residual.\n",
    "\n",
    "![Residuals](assets/loss_function_4.png)\n",
    "\n",
    "For this reason, we minimize the sum of the squares of the residuals! This will be our loss function, commonly called ordinary least squares, or `OLS` for short. This is the same as minimizing the mean squared error of the predictions on the training set.\n",
    "\n",
    "When we call fit on a linear regression model in `scikit-learn`, it performs `OLS` under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression in higher dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have two features and one target, a line is of the form: y = a<sub>1</sub>x<sub>1</sub> + a<sub>2</sub>x<sub>2</sub> + b, so to fit a linear regression model is to specify three variables: a<sub>1</sub>, a<sub>2</sub> and b.\n",
    "    \n",
    "In higher dimensions, that is, when we have more than one or two features, a line is of this form y = a<sub>1</sub>x<sub>1</sub> + a<sub>2</sub>x<sub>2</sub> + ... + a<sub>n</sub>x<sub>n</sub> + b, so fitting a linear regression model is to specify a coefficient, a<sub>i</sub>, for each feature, as well as the variable, b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scikit-learn` API works the same in this case: we pass the `.fit()` method two arrays: one containing the features, the other the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit & predict for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will fit linear regression and predict life expectancy using just one feature. In this example, we will use the `'fertility'` feature of the Gapminder dataset. Since the goal is to predict life expectancy, the target variable here is `'life'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatter plot with `'fertility'` on the x-axis and `'life'` on the y-axis has been generated bellow. As we can see, there is a strong negative correlation, so a linear regression should be able to capture this trend. Our job is to fit linear regression and then predict the life expectancy, overlaying these predicted values on the plot to generate a regression line. We will also compute and print the R<sup>2</sup> score using scikit-learn's `.score()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:56.364834Z",
     "start_time": "2020-02-06T18:03:56.229195Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(x='fertility', y='life', data=gapminder[['fertility', 'life']])\n",
    "plt.xlabel('Life Expectancy')\n",
    "plt.ylabel('Fertility')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:57.008614Z",
     "start_time": "2020-02-06T18:03:56.491907Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:57.147237Z",
     "start_time": "2020-02-06T18:03:57.135271Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the regressor: reg\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:57.287262Z",
     "start_time": "2020-02-06T18:03:57.274192Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the prediction space\n",
    "prediction_space = np.linspace(min(X), max(X)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:57.429089Z",
     "start_time": "2020-02-06T18:03:57.415127Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model to the data\n",
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:57.632569Z",
     "start_time": "2020-02-06T18:03:57.625597Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute predictions over the prediction space: y_pred\n",
    "y_pred = reg.predict(prediction_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:57.882175Z",
     "start_time": "2020-02-06T18:03:57.870175Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print R^2 \n",
    "print(reg.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:03:58.303879Z",
     "start_time": "2020-02-06T18:03:58.166168Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot regression line\n",
    "plt.scatter(x='fertility', y='life', data=gapminder[['fertility', 'life']])\n",
    "plt.xlabel('Life Expectancy')\n",
    "plt.ylabel('Fertility')\n",
    "\n",
    "plt.plot(prediction_space, y_pred, color='black', linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test/split for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned in Lesson 1, train and test sets are vital to ensure that a supervised learning model is able to generalize well to new data. This was true for classification models and is equally true for linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will split the Gapminder dataset into training and testing sets and then fit and predict a linear regression overall features. In addition to computing the R<sup>2</sup> score, we will also compute the Root Mean Squared Error (RMSE), which is another commonly used metric to evaluate regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:00.159130Z",
     "start_time": "2020-02-06T18:04:00.142202Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:00.675046Z",
     "start_time": "2020-02-06T18:04:00.660087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create arrays for features and target variable\n",
    "y = gapminder['life'].values\n",
    "X = gapminder.drop(['life', 'Region'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:00.926375Z",
     "start_time": "2020-02-06T18:04:00.914407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:01.327099Z",
     "start_time": "2020-02-06T18:04:01.312136Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the regressor: reg_all\n",
    "reg_all = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:02.789074Z",
     "start_time": "2020-02-06T18:04:02.776108Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the regressor to the training data\n",
    "reg_all.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:03.663606Z",
     "start_time": "2020-02-06T18:04:03.656624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict on the test data: y_pred\n",
    "y_pred = reg_all.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:04.269140Z",
     "start_time": "2020-02-06T18:04:04.261173Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute and print R^2 and RMSE\n",
    "print(\"R^2: {}\".format(reg_all.score(X_test, y_test)))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that generally, we will never use linear regression out of the box like this; we will most likely wish to use regularization, which we'll see next and which places further constraints on the model coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can spot a potential pitfall of this process: if we're computing R squared on our test set, the R squared returned is dependent on the way that we split up the data! The data points in the test set may have some peculiarities that mean the R squared computed on it is not representative of the model's ability to generalize unseen data.\n",
    "\n",
    "To combat this dependence on what is essentially an arbitrary split, we use a technique called cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by splitting the dataset into five groups or folds. Then we hold out the first fold as a test set, fit our model on the remaining four-folds, predict on the test set, and compute the metric of interest. Next, we hold out the second fold as our test set, fit on the remaining data, predict on the test set, and compute the metric of interest. Then similarly with the third, fourth, and fifth fold.\n",
    "\n",
    "![5 Fold Cross-validation](assets/5_fold.png)\n",
    "\n",
    "As a result, we get five values of R squared from which we can compute statistics of interest, such as the mean and median and 95% confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation and model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we split the dataset into five-folds, we call this process 5-fold cross-validation. If we use 10 folds, it is called 10-fold cross-validation. More generally, if we use `k` folds, it is called k-fold cross-validation or `k-fold CV`.\n",
    "\n",
    "There is, however, a trade-off as using more folds is more computationally expensive. This is because we are fitting and predicting more times. This method avoids the problem of our metric of choice being dependent on the train test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a vital step in evaluating a model. It maximizes the amount of data that is used to train the model, as, during training, the model is not only trained but also tested on all of the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will practice 5-fold cross-validation on the Gapminder data. By default, scikit-learn's `cross_val_score()` function uses R<sup>2</sup> as the metric of choice for regression. Since we are performing 5-fold cross-validation, the function will return 5 scores. Our job is to compute these 5 scores and then take their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:10.821491Z",
     "start_time": "2020-02-06T18:04:10.811521Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:11.555225Z",
     "start_time": "2020-02-06T18:04:11.550269Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a linear regression object: reg\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:12.129103Z",
     "start_time": "2020-02-06T18:04:12.113179Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute 5-fold cross-validation scores: cv_scores\n",
    "cv_scores = cross_val_score(reg, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:12.718765Z",
     "start_time": "2020-02-06T18:04:12.699819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the 5-fold cross-validation scores\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:14.197311Z",
     "start_time": "2020-02-06T18:04:14.184354Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold CV comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is essential, but do not forget that the more folds we use, the more computationally expensive cross-validation becomes. In this example, we will explore this. Our job is to perform 3-fold cross-validation and then 10-fold cross-validation on the Gapminder dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:16.626462Z",
     "start_time": "2020-02-06T18:04:16.613466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a linear regression object: reg\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:17.015522Z",
     "start_time": "2020-02-06T18:04:16.997607Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform 3-fold CV\n",
    "%time cvscores_3 = cross_val_score(reg, X, y, cv=3)\n",
    "print(np.mean(cvscores_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:17.389063Z",
     "start_time": "2020-02-06T18:04:17.356138Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform 10-fold CV\n",
    "%time cvscores_10 = cross_val_score(reg, X, y, cv=10)\n",
    "print(np.mean(cvscores_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why regularize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that what fitting a linear regression does, is minimize a loss function to choose a coefficient a<sub>i</sub> for each feature variable. If we allow these coefficients or parameters to be super large, we can get overfitting. It isn't so easy to see in two dimensions, but when we have loads and loads of features, that is, if our data sits in a high-dimensional space, with large coefficients, it gets easy to predict nearly anything.\n",
    "\n",
    "For this reason, it is common practice to alter the loss function so that it is penalized for large coefficients. This is called regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first type of regularized regression that we will look at is called Ridge regression in which our loss function is the standard OLS loss function plus the squared value of each coefficient multiplied by some constant alpha:\n",
    "$$ Loss function = OLS + \\alpha * \\sum_{i=1}^n a_i^2 $$\n",
    "Thus, when minimizing the loss function to fit our data, models are penalized for coefficients with a large magnitude: large positive and large negative coefficients, that is.\n",
    "\n",
    "Note that alpha is a parameter we need to choose to fit and predict. Essentially, we can select the alpha for which our model performs best. Picking alpha for Ridge regression is similar to picking k in k-NN. This is called hyperparameter tuning, and we'll see much more of this in week 5. \n",
    "\n",
    "This alpha, which may also be called lambda in the wild, can be thought of as a parameter that controls model complexity. Notice that when alpha is equal to zero, we get back OLS. Large coefficients, in this case, are not penalized and the overfitting problem is not accounted for. A very high alpha means that large coefficients are significantly penalized, which can lead to a model that is too simple and ends up underfitting the data.\n",
    "\n",
    "The method of performing Ridge regression with scikit-learn mirrors the other model that we have seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another type of regularized regression call `Lasso` regression, in which our loss function is the standard OLS loss function plus the absolute value of each coefficient multiplied by some constant alpha:\n",
    "$$ Loss function = OLS_{loss function} + \\alpha * \\sum_{i=1}^n |a_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of performing Lasso regression in scikit-learn mirrors Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso regression for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the cool aspects of Lasso regression is that it can be used to select important features of a dataset. This is because it tends to shrink the coefficients of less important features to be precisely zero. The features whose coefficients are not shrunk to zero are `'selected'` by the `Lasso` algorithm.\n",
    "\n",
    "This type of feature selection is essential for machine learning in an industry or business setting because it allows us to communicate significant results to non-technical colleagues. And bosses!\n",
    "\n",
    "The power of reporting essential features from a linear model cannot be overestimated. It is also valuable in science research to identify which factors are significant predictors for various physical phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Example I: Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will fit a lasso regression to the Gapminder data we have been working with and plot the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:26.679016Z",
     "start_time": "2020-02-06T18:04:26.670007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Lasso\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:26.989021Z",
     "start_time": "2020-02-06T18:04:26.986053Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a lasso regressor: lasso\n",
    "lasso = Lasso(alpha=0.4, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:27.270255Z",
     "start_time": "2020-02-06T18:04:27.256293Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the regressor to the data\n",
    "lasso.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:27.579976Z",
     "start_time": "2020-02-06T18:04:27.570967Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute and print the coefficients\n",
    "lasso_coef = lasso.coef_\n",
    "print(lasso_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:04:28.484571Z",
     "start_time": "2020-02-06T18:04:28.356943Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the coefficients\n",
    "columns = ['population', 'fertility', 'HIV', 'CO2', 'BMI_male', 'GDP', 'BMI_female', 'child_mortality']\n",
    "plt.plot(range(len(columns)), lasso_coef)\n",
    "plt.xticks(range(len(columns)), columns, rotation=60)\n",
    "plt.margins(0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Example II: Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso is excellent for feature selection, but when building regression models, Ridge regression should be our first choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that lasso performs regularization by adding to the loss function a penalty term of the absolute value of each coefficient multiplied by some alpha. This is also known as `L1` regularization because the regularization term is the `L1` norm of the coefficients. This is not the only way to regularize, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead, we took the sum of the squared values of the coefficients multiplied by some alpha-like in Ridge regression - we would be computing the `L2` norm. In this example, we will practice fitting ridge regression models over a range of different alphas, and plot cross-validated R<sup>2</sup> scores for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:07:02.952479Z",
     "start_time": "2020-02-06T18:07:02.941542Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_plot(cv_scores, cv_scores_std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(alpha_space, cv_scores)\n",
    "\n",
    "    std_error = cv_scores_std / np.sqrt(10)\n",
    "\n",
    "    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n",
    "    ax.set_ylabel('CV Score +/- Std Error')\n",
    "    ax.set_xlabel('Alpha')\n",
    "    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n",
    "    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n",
    "    ax.set_xscale('log')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:07:11.679141Z",
     "start_time": "2020-02-06T18:07:11.670164Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:07:13.366671Z",
     "start_time": "2020-02-06T18:07:13.358716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup the array of alphas and lists to store scores\n",
    "alpha_space = np.logspace(-4, 0, 50)\n",
    "ridge_scores = []\n",
    "ridge_scores_std = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:07:18.148097Z",
     "start_time": "2020-02-06T18:07:18.143111Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a ridge regressor: ridge\n",
    "ridge = Ridge(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note** The argument `normalize`: setting this equal to `True` ensures that all our variables are on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:07:35.723314Z",
     "start_time": "2020-02-06T18:07:35.275122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute scores over range of alphas\n",
    "for alpha in alpha_space:\n",
    "\n",
    "    # Specify the alpha value to use: ridge.alpha\n",
    "    ridge.alpha = alpha\n",
    "    \n",
    "    # Perform 10-fold CV: ridge_cv_scores\n",
    "    ridge_cv_scores = cross_val_score(ridge, X, y, cv=10)\n",
    "    \n",
    "    # Append the mean of ridge_cv_scores to ridge_scores\n",
    "    ridge_scores.append(np.mean(ridge_cv_scores))\n",
    "    \n",
    "    # Append the std of ridge_cv_scores to ridge_scores_std\n",
    "    ridge_scores_std.append(np.std(ridge_cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T18:07:40.757807Z",
     "start_time": "2020-02-06T18:07:40.266147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display the plot\n",
    "display_plot(ridge_scores, ridge_scores_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best value for `alpha` is 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**[Week 3 - Supervised Learning](https://radu-enuca.gitbook.io/ml-challenge/supervised-learning)**\n",
    "\n",
    "*Have questions or comments? Visit the ML Challenge Mattermost Channel.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
