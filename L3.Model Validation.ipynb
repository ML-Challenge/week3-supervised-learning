{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ML-Challenge/week3-supervised-learning/blob/master/L3.Model%20Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:50.078504Z",
     "start_time": "2020-02-08T15:22:50.064544Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download lesson datasets\n",
    "# Required if you're using Google Colab\n",
    "!wget \"https://github.com/ML-Challenge/week3-supervised-learning/raw/master/datasets.zip\"\n",
    "!unzip -o datasets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.385446Z",
     "start_time": "2020-02-08T15:22:50.080498Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import utils\n",
    "# We'll be using this module throughout the lesson\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.606824Z",
     "start_time": "2020-02-08T15:22:51.387460Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# and setting the size of all plots.\n",
    "plt.rcParams['figure.figsize'] = [11, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Creating train, test and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a holdout dataset as any data that is not used for training and is only used to assess model performance. The available data is split into two datasets. One used for training, and one that is simply off-limits while we are training our models, called a test (or holdout) dataset.\n",
    "\n",
    "This step is vital to model validation and is the number one step we can take to ensure our model's performance. We use the holdout sample as a testing dataset so that we can have an unbiased estimate for our model's performance after we are completely done training. \n",
    "\n",
    "Generally, a good rule of thumb is using an `80:20` split. This equates to setting aside twenty percent of the data for the test set and using the rest for training. We might choose to use more training data when the overall data is limited (`90:10`), or less training data if the modeling method is computationally expensive (`70:30`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for preliminary testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the test set is off-limits until we are completely done training, but what do we do when testing model parameters? For example, if we run a random forest model with 100 trees and one with 1000 trees, which dataset do we use to test these results? When testing parameters, tuning hyperparameters, or anytime we are frequently evaluating model performance, we need to create a second holdout sample, called the validation dataset.\n",
    "\n",
    "For this dataset, the available data is the original training dataset, which is then split in the same manner used to split the original complete dataset. We use the validation sample to asses our model's performance when using different parameter values.\n",
    "\n",
    "To create both holdout samples, the testing, and the validation datasets, we use scikit-learn's `train_test_split()` function twice. The first call will create training and testing datasets like normal\n",
    "\n",
    "```\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "```\n",
    "\n",
    "The second call we split this so-called temporary training dataset into the final training and validation datasets.\n",
    "\n",
    "```\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size = 0.25, random_state = 42)\n",
    "```\n",
    "\n",
    "In the above example, we used first an 80/20 split to create the test set. With the 80% training dataset, we used a 75/25 split to create a validation dataset. This is leaving us with 60% of the data for training, 20% for validation, and 20% for testing.\n",
    "\n",
    "![Train, test, validation](assets/train_test_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Accuracy metrics: regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned about holdout samples, let's discuss accuracy metrics used when validating models - starting with regression models. Remember, regression models, are built for continuous variables. This could be predicting the number of points a player will score tomorrow, or the number of puppies a dog is about to have!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean absolute error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the performance of a regression model, we can use the mean absolute error. It is the simplest and most intuitive error metric and is the average absolute difference between the predictions $y_i$ and the actual values $\\hat{y}$:\n",
    "\n",
    "$$ MAE = \\frac{\\sum_{i=1}^n |y_i - \\hat{y}_i|}{n} $$\n",
    "\n",
    "If a dog had six puppies, but we predicted only four, the absolute difference would be two. This metric treats all points equally and is not sensitive to outliers. When dealing with applications where we don't want large errors to have a major impact, the mean absolute error can be used. An example could be predicting the car's monthly gas bill, when a one-time road trip may have caused an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communicating modeling results can be difficult. However, most clients people that on average, a predictive model was off by some number. This makes explaining the mean absolute error easy. For example, when predicting the number of wins for a basketball team, if we predict 42, and they end up with 40, we can easily explain that the error was two wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have two arrays. `y_test`, the true number of wins for all 30 NBA teams in 2017, and `predictions`, which contains a prediction for each team. Let's calculate the MAE both manually using `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.622476Z",
     "start_time": "2020-02-08T15:22:51.607820Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.638440Z",
     "start_time": "2020-02-08T15:22:51.623450Z"
    }
   },
   "outputs": [],
   "source": [
    "# Manually calculate the MAE\n",
    "n = len(utils.nba_predictions)\n",
    "mae_one = sum(abs(utils.nba_y_test - utils.nba_predictions)) / n\n",
    "print('With a manual calculation, the error is {}'.format(mae_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.653366Z",
     "start_time": "2020-02-08T15:22:51.639434Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use scikit-learn to calculate the MAE\n",
    "mae_two = mean_absolute_error(utils.nba_y_test, utils.nba_predictions)\n",
    "print('Using scikit-lean, the error is {}'.format(mae_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean squared error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the mean squared error (MSE). It is the most widely used error metric for regression models. It is calculated similarly to the mean absolute error, but this time we square the difference term.\n",
    "\n",
    "$$ MSE = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n} $$\n",
    "\n",
    "The MSE allows larger errors to have a larger impact on the model. Using the previous car example, if we knew once a year we might go on a road trip, we might expect to have a large error occasionally and would want our model to pick up on these trips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the 2017 NBA predictions again. Every year, there are at least a couple of NBA teams that win way more games than expected. If we use the MAE, this accuracy metric does not reflect the bad predictions as much as if we use the MSE. Squaring the large errors from bad predictions will make the accuracy even bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.669354Z",
     "start_time": "2020-02-08T15:22:51.657356Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.684279Z",
     "start_time": "2020-02-08T15:22:51.671316Z"
    }
   },
   "outputs": [],
   "source": [
    "n = len(utils.nba_predictions)\n",
    "# Finish the manual calculation of the MSE\n",
    "mse_one = sum((utils.nba_y_test - utils.nba_predictions)**2) / n\n",
    "print('With a manual calculation, the error is {}'.format(mse_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.699236Z",
     "start_time": "2020-02-08T15:22:51.686274Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the scikit-learn function to calculate MSE\n",
    "mse_two = mean_squared_error(utils.nba_y_test, utils.nba_predictions)\n",
    "print('Using scikit-lean, the error is {}'.format(mse_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE vs MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking between the MAE and the MSE comes down to the application. These results are in different units, though, and should not be directly compared!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on data subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In professional basketball, there are two conferences, the East and the West. Coaches and fans often only care about how teams in their conference will do this year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been working on an NBA prediction model. We would like to determine if the predictions were better for the East or West conference. We added a third array to the data called `utils.nba_labels`, which contains an \"E\" for the East teams, and a \"W\" for the West."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.715192Z",
     "start_time": "2020-02-08T15:22:51.701243Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find the East conference teams\n",
    "east_teams = utils.nba_labels == \"E\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.730150Z",
     "start_time": "2020-02-08T15:22:51.717187Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create arrays for the true and predicted values\n",
    "true_east = utils.nba_y_test[east_teams]\n",
    "preds_east = utils.nba_predictions[east_teams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.745108Z",
     "start_time": "2020-02-08T15:22:51.731147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the accuracy metrics\n",
    "print('The MAE for East teams is {}'.format(mean_absolute_error(true_east, preds_east)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.761063Z",
     "start_time": "2020-02-08T15:22:51.746105Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create arrays for the true and predicted values\n",
    "true_west = utils.nba_y_test[~east_teams]\n",
    "preds_west = utils.nba_predictions[~east_teams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.776021Z",
     "start_time": "2020-02-08T15:22:51.762060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the accuracy metrics\n",
    "print('The MAE for West teams is {}'.format(mean_absolute_error(true_west, preds_west)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the Western conference predictions were about two games better on average. Over the past few seasons, the Western teams have generally won the same number of games as the experts have predicted. Teams in the East are just not as predictable as those in the West."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already understand classification models; now, let's look at their accuracy metrics. Classification accuracy metrics are quite a bit different than regression ones. Remember, with classification models; we are predicting what category an observation falls into. There are a lot of accuracy metrics available: `precision`, `recall` (also called `sensitivity`), `accuracy`, `specificity`, `F1-Score`, and it's variations and several others.\n",
    "\n",
    "We will focus on precision, recall, and accuracy as each of these are easy to understand and have very practical applications. One way to calculate these metrics is to use the values from the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making predictions, especially if there is a binary outcome, this matrix is one of the first outputs we should preview. When we have a binary outcome, the confusion matrix is a 2x2 matrix that shows how our predictions faired across the two outcomes. For example, for predictions of `0` that were actually `0` (or true negatives), we look at the `0,0` square of the matrix.\n",
    "\n",
    "![Confusion matrix](assets/confusion_matrix.png)\n",
    "\n",
    "All of the above accuracy metrics can be calculated using the values from this matrix, and it is a great way to visualize the initial results of our classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a confusion matrix using `scikit-learn`'s function `confusion_matrix()`. When dealing with binary data, this will print out a 2x2 array which represents the confusion matrix. In this matrix, the row index represents the true category, and the column index represents the predicted category. Therefore, the 1,0 entry of the array represents the number of true `1s` that were predicted to be `0`, or 8 in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is the easiest metric to understand and represents the overall ability of the model to correctly predict the correct classification. Using the confusion matrix, we add the values were predicted `0` and are actually `0` (which are called true negatives), to the values predicted to be `1` that are `1` (called true positives), and then divide by the total number of observations:\n",
    "\n",
    "$$ Accuracy = \\frac{TN + TP}{TN + FP + FN + TP} = \\frac{23 + 62}{23 + 7 + 8 + 62} = 0.85 $$\n",
    "\n",
    "In this case, our accuracy was 85%. In this example, we can associate a true positive as predicted 1's that are also 1's. However, if our categories were win or loss, we might associate a true positive as the number of predicted wins that won."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is precision or the number of true positives out of all predicted positive values:\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP} = \\frac{62}{62 + 7} = 0.90 $$\n",
    "\n",
    "Precision is used when we don't want to over-predict positive values. It costs $2,000 to fly-in potential new employees, a company may only have on-campus interviews with individuals that they believe are going to join their company. In the example, almost 9 out of 10 predicted 1's would have joined the company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall metric is about finding all positive values:\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP + FN} = \\frac{62}{62 + 8} = 0.885 $$\n",
    "\n",
    "Here we correctly predicted 62 true positives and had 8 false negatives. Our recall is 62 out of 70. The recall is used when we can't afford to miss any positive values. For example, even if a patient has a small chance of having cancer, we may want to give them additional tests. The cost of missing a patient who has cancer is far higher than the cost of additional screenings for that patient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, precision, and recall are called similarly. Use the desired accuracy metric function and provide the 'true' and predicted values.\n",
    "\n",
    "```\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "accuracy_score(y_test, test_predictions) # .85\n",
    "precision_score(y_test, test_predictions) # .8986\n",
    "recall_score(y_test, test_predictions) # .8857\n",
    "```\n",
    "\n",
    "A single value will be produced as a result. In this example, we got the same values that we calculated using the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices, again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a confusion matrix in Python is simple. The biggest challenge will be making sure is understanding the orientation of the matrix. The following example illustrates the `sklearn` implementation of confusion matrices. Here, we have created a `classifier` model using the `tic_tac_toe` dataset to predict outcomes of 0 (loss) or 1 (a win) for Player One."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.867795Z",
     "start_time": "2020-02-08T15:22:51.777019Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create predictions\n",
    "test_predictions = utils.classifier.predict(utils.X_test)\n",
    "\n",
    "# Create and print the confusion matrix\n",
    "cm = confusion_matrix(utils.y_test, test_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.883718Z",
     "start_time": "2020-02-08T15:22:51.868792Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the true positives (actual 1s that were predicted 1s)\n",
    "print(\"The number of true positives is: {}\".format(cm[1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision vs. recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy metrics we use to evaluate our model should always be based on the specific application. For this example, let's assume we are a really sore loser when it comes to playing Tic-Tac-Toe, but only when we are certain that we are going to win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the appropriate accuracy metric? Precision or recall? Keep in mind, if we think we are going to win, we better win!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.899673Z",
     "start_time": "2020-02-08T15:22:51.885714Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.914632Z",
     "start_time": "2020-02-08T15:22:51.900703Z"
    }
   },
   "outputs": [],
   "source": [
    "score = precision_score(utils.y_test, test_predictions)\n",
    "\n",
    "# Print the final result\n",
    "print(\"The precision value is {0:.2f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## The bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to identify when we have a good fitting model. One way to do this is to consider bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance occurs when a model pays too close attention to the training data and fails to generalize to the testing data. These models perform well on only the training data, but not on the testing data, and are considered to overfit.\n",
    "\n",
    "Overfitting occurs when our model starts to attach meaning to the noise in the training data. In the following graphic, we can see the natural quadratic shape of the orange dots. However, our blue prediction line is hugging the data and would likely not extend well to new orange dots.\n",
    "\n",
    "![Overfitting](assets/overfitting.png)\n",
    "\n",
    "Overfitting is easy to identify, though, as the training error will be a lot lower than the testing error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second term, `Bias`, occurs when the model fails to find the relationships between the data and the response value. Bias leads to high errors in both the training and testing datasets and is associated with an underfit model. Underfitting occurs when the model could not find the underlying patterns available in the data. In this example, we have the average of the actual values acting as our prediction.\n",
    "\n",
    "![Underfitting](assets/underfitting.png)\n",
    "\n",
    "Underfitting is more difficult to identify because the training and testing errors will both be high, and it's difficult to know if we got the most out of the data, or if we can improve the testing error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our model is getting the most out of the training data, while still performing on the testing data, we have optimal performance. Notice how the blue line is matching the natural quadratic shape of the data, and that is not touching every orange dot. The blue line is a well-fit prediction line for future data.\n",
    "\n",
    "![Optimal performance](assets/optimal_performance.png)\n",
    "\n",
    "So how do we tell if we have a good fit, or if we are underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## More on metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification, we've seen that we can use accuracy, the fraction of correctly classified samples, to measure model performance. However, accuracy is not always a useful metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class imbalance example: Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a spam classification problem in which 99% of emails are real, and only 1% are spam. We could build a model that classifies all emails as real; this model would be correct 99% of the time and thus have an accuracy of 99%, which sounds great. However, this naive classifier does a horrible job of predicting spam: it never predicts spam at all, so it utterly fails at its original purpose.\n",
    "\n",
    "The situation when one class is more frequent is called class imbalance because the class of real emails contains way more instances than the class of spam. This is a widespread situation in practice and requires a more nuanced metric to assess the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosing classification predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a binary classification task, such as our spam email example, we can draw up the 2x2 confusion matrix. Note that correctly labeled spam emails are the true positives and correctly labeled real emails as true negatives. While incorrectly labeled, spam will be referred to as false negatives and incorrectly labeled real emails as false positives.\n",
    "\n",
    "Usually, the \"class of interest\" is called the positive class. As we are trying to detect spam, this makes spam the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1-Score is defined as two times the product of the precision and recall divided by the sum of the precision and recall; in other words, it's the harmonic mean of precision and recall:\n",
    "\n",
    "$$ F_{1score} = 2 * \\frac{Precision * Recall}{Precision + Recall} $$\n",
    "\n",
    "To put it in everyday language, high precision means that our classifier had a low false-positive rate; this is, not many real emails were predicted as being spam. Intuitively, high recall means that our classifier predicted most positive or spam emails correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll work with the [PIMA Indians](https://www.kaggle.com/uciml/pima-indians-diabetes-database) dataset obtained from the UCI Machine Learning Repository. The goal is to predict whether or not a given female patient will contract diabetes based on features such as BMI, age, and the number of pregnancies. Therefore, it is a binary classification problem. A target value of `0` indicates that the patient does not have diabetes. In contrast, a value of `1` indicates that the patient does have diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:22:51.930587Z",
     "start_time": "2020-02-08T15:22:51.915629Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.diabetes.head()                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Lessons 1 and 2, the dataset has been preprocessed to deal with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:23:40.861441Z",
     "start_time": "2020-02-08T15:23:40.851503Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.diabetes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our job is to train a k-NN classifier to the data and evaluate its performance by generating a confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:26:08.353157Z",
     "start_time": "2020-02-08T15:26:08.346205Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:26:19.598395Z",
     "start_time": "2020-02-08T15:26:19.591415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create training and test set\n",
    "X = utils.diabetes.drop('diabetes', axis=1)\n",
    "y = utils.diabetes['diabetes']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:26:20.013217Z",
     "start_time": "2020-02-08T15:26:19.997309Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a k-NN classifier: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:26:20.463514Z",
     "start_time": "2020-02-08T15:26:20.449552Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:26:25.478826Z",
     "start_time": "2020-02-08T15:26:25.463830Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the labels of the test data: y_pred\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T15:26:35.607121Z",
     "start_time": "2020-02-08T15:26:35.588213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support gives the number of samples of the true response that lie in that class. The precision, recall, and f1-score columns, then, gave the respective metrics for that particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Logistic regression and the ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to introduce another model to our classification arsenal: logistic regression. Despite its name, logistic regression is used in classification problems, not regression problems. We won't go into the mathematical details here, see the optional mathematics sections for that. However, we will provide an intuition towards how logistic regression or log reg works for binary classification, that is when we have two possible labels for the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given one feature, log reg will output a probability, `p`, with respect to the target variable. If `p` is greater than 0.5, we label the data as `'1'`; if `p` is less than 0.5, we label it as `'0'`.\n",
    "\n",
    "![Logistic regression](assets/log_reg.png)\n",
    "\n",
    "Note that log reg produces a linear decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using logistic regression in `scikit-learn` follows exactly the same formula that we now know so well: perform the necessary imports, instantiate the classifier, split the data into training and test sets, fit the model on the training data, and predict on the test set.\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, logistic regression uses a threshold of 0.5, a threshold that defines our model. Note that this is not particular for log reg but also could be used for KNN, for example. Now, what happens as we vary this threshold? In particular, what happens to the true positive and false positive rates as we vary the threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the threshold equals 0, the model predicts '1' for all the data, which means the true positive rate is equal to the false positive rate is equal to 1.\n",
    "\n",
    "![p=0](assets/roc_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the threshold equals 1, the model predicts '0' for all the data, which means both true and false positive rates are zero.\n",
    "\n",
    "![p=1](assets/roc_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we vary the threshold between these two extremes, we get a series of different false positive and true positive rates.\n",
    "\n",
    "![p varies](assets/roc_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of points we get when trying all possile threshold is called the `Receiver Operating Characteristic curve` or `ROC curve`\n",
    "\n",
    "![ROC](assets/roc_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the ROC curve, we import `roc_curve` from `sklearn.metrics`; we then call the function `roc_curve()`; the first argument is given by the actual labels, the second by the predicted probabilities. We unpack the result into three variables: false positive rate, FPR; true positive rate TPR; and the thresholds. \n",
    "\n",
    "```\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Logistic Regression ROC curve')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "We can then plot the FPR and TPR using `pyplot's` plot function to produce a figure as this:\n",
    "\n",
    "![Plotted ROC](assets/roc_5.png)\n",
    "\n",
    "We used the predicted probabilities of the model assigning a value of '1' to the obeservation in question. This is because to compute ROC we don't merely want the predictions on the test set, but we want the probability that our log reg model outputs before using a threshold to predict the label. To do this we apply the method `.predict_proba()` to the model and pass it the test data. `.predict_proba()` returns an array with two columns: each column contains the probabilities for the respective target values. We choose the second column, the one with index 1, that is, the probabilities of the predicted labels being '1'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build our first logistic regression model! As already shown, scikit-learn makes it very easy to try different models, since the Train-Test-Split/Instantiate/Fit/Predict paradigm applies to all classifiers and regressors - which are known in scikit-learn as 'estimators'. We'll see this now as we train a logistic regression model on exactly the same data as in the previous example. Will it outperform k-NN? There's only one way to find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T16:06:21.134188Z",
     "start_time": "2020-02-08T16:06:21.117244Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T16:11:03.331792Z",
     "start_time": "2020-02-08T16:11:03.316806Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the classifier: logreg\n",
    "logreg = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T16:11:04.594548Z",
     "start_time": "2020-02-08T16:11:04.575569Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the classifier to the training data\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T16:11:18.070732Z",
     "start_time": "2020-02-08T16:11:18.053754Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T16:11:22.908397Z",
     "start_time": "2020-02-08T16:11:22.888451Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is used in a variety of machine learning applications. It is a vital part of our data science toolbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification reports and confusion matrices are great methods to evaluate model performance quantitatively. At the same time, ROC curves provide a way to evaluate models visually. Most classifiers in scikit-learn have a `.predict_proba()` method, which returns the probability of a given sample being in a particular class. Having built a logistic regression model, we'll now evaluate its performance by plotting a ROC curve. In doing so, we'll make use of the `.predict_proba()` method and become familiar with its functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T16:13:46.046059Z",
     "start_time": "2020-02-08T16:13:46.030135Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T16:13:50.540785Z",
     "start_time": "2020-02-08T16:13:50.527794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T16:14:54.834760Z",
     "start_time": "2020-02-08T16:14:54.826760Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T16:14:01.452807Z",
     "start_time": "2020-02-08T16:14:01.281277Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area under the ROC curve (AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the question is: given the ROC curve, can we extract a metric of interest? Consider the following: the larger the area under the ROC curve, the better our model is! \n",
    "\n",
    "The way to think about this is the following: if we had a model that produced a ROC curve that had a single point at 1,0, the upper left corner, representing a true positive rate of one and a false positive rate of zero, this would be a great model!\n",
    "\n",
    "![Perfect ROC](assets/roc_6.png)\n",
    "\n",
    "For this reason, the area under the ROC, commonly denoted as AUC, is another popular metric for classification models.\n",
    "\n",
    "![Perfect AUC](assets/auc_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have a binary classifier that, in fact, is just randomly making guesses. It would be correct approximately 50% of the time, and the resulting ROC curve would be a diagonal line in which the True Positive Rate and False Positive Rate are always equal. The Area under this ROC curve would be 0.5. This is one way in which the AUC is an informative metric to evaluate a model. If the AUC is greater than 0.5, the model is better than random guessing. Always a good sign!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll calculate AUC scores using the `roc_auc_score()` function from `sklearn.metrics` as well as by performing cross-validation on the diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T16:24:29.618103Z",
     "start_time": "2020-02-08T16:24:29.609127Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T16:24:36.892942Z",
     "start_time": "2020-02-08T16:24:36.874957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T16:24:48.213467Z",
     "start_time": "2020-02-08T16:24:48.191494Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute and print AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T16:24:55.222362Z",
     "start_time": "2020-02-08T16:24:55.173462Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute cross-validated AUC scores: cv_auc\n",
    "cv_auc = cross_val_score(logreg, X, y , cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T16:25:02.624225Z",
     "start_time": "2020-02-08T16:25:02.617244Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print list of AUC scores\n",
    "print(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**[Week 3 - Supervised Learning](https://radu-enuca.gitbook.io/ml-challenge/supervised-learning)**\n",
    "\n",
    "*Have questions or comments? Visit the ML Challenge Mattermost Channel.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "337px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
